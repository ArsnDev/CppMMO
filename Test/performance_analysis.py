#!/usr/bin/env python3
"""
CppMMO ì„œë²„ ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬
pandasì™€ matplotlibì„ í™œìš©í•˜ì—¬ ì¸ì›ìˆ˜ë³„ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import json
import glob
import os
from datetime import datetime
import seaborn as sns
from pathlib import Path
import argparse

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class PerformanceAnalyzer:
    def __init__(self, data_directory='.'):
        self.data_directory = data_directory
        self.performance_data = {}
        self.csv_data = {}
        
    def load_performance_data(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ JSON íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        json_files = glob.glob(os.path.join(self.data_directory, 'performance_results_*.json'))
        
        print(f"ë°œê²¬ëœ ì„±ëŠ¥ ê²°ê³¼ íŒŒì¼: {len(json_files)}ê°œ")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    scenario = data.get('scenario', 'unknown')
                    client_count = data.get('client_count', 0)
                    
                    key = f"{scenario}_{client_count}"
                    self.performance_data[key] = data
                    
                    print(f"ë¡œë“œ: {json_file} - {scenario} ì‹œë‚˜ë¦¬ì˜¤, {client_count}ëª…")
                    
            except Exception as e:
                print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({json_file}): {e}")
    
    def load_csv_data(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        csv_files = glob.glob(os.path.join(self.data_directory, 'performance_test_*.csv'))
        
        print(f"ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                filename = os.path.basename(csv_file)
                self.csv_data[filename] = df
                
                print(f"ë¡œë“œ: {filename} - {len(df)} í–‰")
                
            except Exception as e:
                print(f"CSV ë¡œë“œ ì˜¤ë¥˜ ({csv_file}): {e}")
    
    def create_client_count_scenarios(self):
        """ë‹¤ì–‘í•œ í´ë¼ì´ì–¸íŠ¸ ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        client_counts = [10, 25, 50, 100, 200, 300, 500]
        scenarios = ['basic', 'stress']
        
        test_configs = []
        for scenario in scenarios:
            for client_count in client_counts:
                config = {
                    'scenario': scenario,
                    'clients': client_count,
                    'duration': 120 if scenario == 'basic' else 180,  # ê¸°ë³¸ 2ë¶„, ìŠ¤íŠ¸ë ˆìŠ¤ 3ë¶„
                    'movement_interval': 0.05 if scenario == 'basic' else 0.033,
                    'chat_interval': 10.0 if scenario == 'basic' else 5.0
                }
                test_configs.append(config)
        
        return test_configs
    
    def extract_performance_metrics(self):
        """ì„±ëŠ¥ ë°ì´í„°ì—ì„œ ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        metrics = []
        
        for key, data in self.performance_data.items():
            try:
                scenario = data.get('scenario', 'unknown')
                client_count = data.get('client_count', 0)
                final_stats = data.get('final_stats', {})
                
                # ì²˜ë¦¬ëŸ‰ ì§€í‘œ
                throughput = final_stats.get('throughput', {})
                packets_per_sec = throughput.get('packets_per_sec', 0)
                mbps_sent = throughput.get('mbps_sent', 0)
                mbps_received = throughput.get('mbps_received', 0)
                
                # ì§€ì—°ì‹œê°„ ì§€í‘œ
                latency = final_stats.get('latency', {})
                avg_latency = latency.get('avg', 0)
                p95_latency = latency.get('p95', 0)
                p99_latency = latency.get('p99', 0)
                
                # RTT ì§€í‘œ
                rtt = final_stats.get('rtt', {})
                avg_rtt = rtt.get('avg', 0)
                
                # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
                system = final_stats.get('system_resources', {})
                cpu_usage = system.get('cpu_usage', 0)
                memory_usage = system.get('memory_usage', 0)
                
                # ì•ˆì •ì„± ì§€í‘œ
                errors = final_stats.get('errors', {})
                error_rate = errors.get('error_rate_percent', 0)
                
                # ì—°ê²° ì§€í‘œ
                connections = final_stats.get('connections', {})
                connected_clients = connections.get('connected', 0)
                in_zone_clients = connections.get('in_zone', 0)
                
                # ì¢…í•© ì ìˆ˜
                overall_score = data.get('overall_score', 0)
                
                metric = {
                    'scenario': scenario,
                    'client_count': client_count,
                    'packets_per_sec': packets_per_sec,
                    'mbps_sent': mbps_sent,
                    'mbps_received': mbps_received,
                    'avg_latency_ms': avg_latency,
                    'p95_latency_ms': p95_latency,
                    'p99_latency_ms': p99_latency,
                    'avg_rtt_ms': avg_rtt,
                    'cpu_usage_percent': cpu_usage,
                    'memory_usage_percent': memory_usage,
                    'error_rate_percent': error_rate,
                    'connected_clients': connected_clients,
                    'in_zone_clients': in_zone_clients,
                    'connection_success_rate': (connected_clients / client_count * 100) if client_count > 0 else 0,
                    'overall_score': overall_score,
                    'timestamp': data.get('timestamp', ''),
                    'test_duration': data.get('test_duration', 0)
                }
                
                metrics.append(metric)
                
            except Exception as e:
                print(f"ë©”íŠ¸ë¦­ ì¶”ì¶œ ì˜¤ë¥˜ ({key}): {e}")
        
        return pd.DataFrame(metrics)
    
    def create_scalability_charts(self, df, output_dir='performance_charts'):
        """ì¸ì›ìˆ˜ë³„ í™•ì¥ì„± ì°¨íŠ¸ ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìƒ‰ìƒ ì •ì˜
        colors = {'basic': '#2E86AB', 'stress': '#F24236', 'extreme': '#F18F01'}
        
        # 1. ì²˜ë¦¬ëŸ‰ vs í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        plt.figure(figsize=(15, 10))
        
        plt.subplot(2, 3, 1)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            plt.plot(scenario_data['client_count'], scenario_data['packets_per_sec'], 
                    marker='o', label=f'{scenario.title()}', color=colors.get(scenario, '#333333'), linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('Packets per Second')
        plt.title('Throughput Scalability')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. ì§€ì—°ì‹œê°„ vs í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        plt.subplot(2, 3, 2)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            plt.plot(scenario_data['client_count'], scenario_data['p95_latency_ms'], 
                    marker='s', label=f'{scenario.title()} P95', color=colors.get(scenario, '#333333'), linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('P95 Latency (ms)')
        plt.title('Latency Scalability')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.yscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ë³€ê²½
        
        # 3. CPU ì‚¬ìš©ë¥  vs í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        plt.subplot(2, 3, 3)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            plt.plot(scenario_data['client_count'], scenario_data['cpu_usage_percent'], 
                    marker='^', label=f'{scenario.title()}', color=colors.get(scenario, '#333333'), linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('CPU Usage (%)')
        plt.title('CPU Usage Scalability')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 100)
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  vs í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        plt.subplot(2, 3, 4)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            plt.plot(scenario_data['client_count'], scenario_data['memory_usage_percent'], 
                    marker='d', label=f'{scenario.title()}', color=colors.get(scenario, '#333333'), linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('Memory Usage (%)')
        plt.title('Memory Usage Scalability')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 100)
        
        # 5. ì—°ê²° ì„±ê³µë¥  vs í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        plt.subplot(2, 3, 5)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            plt.plot(scenario_data['client_count'], scenario_data['connection_success_rate'], 
                    marker='v', label=f'{scenario.title()}', color=colors.get(scenario, '#333333'), linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('Connection Success Rate (%)')
        plt.title('Connection Reliability')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 105)
        
        # 6. ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ vs í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        plt.subplot(2, 3, 6)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            plt.plot(scenario_data['client_count'], scenario_data['overall_score'], 
                    marker='*', markersize=8, label=f'{scenario.title()}', color=colors.get(scenario, '#333333'), linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('Overall Score')
        plt.title('Overall Performance Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 100)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/scalability_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"í™•ì¥ì„± ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {output_dir}/scalability_analysis.png")
    
    def create_performance_heatmap(self, df, output_dir='performance_charts'):
        """ì„±ëŠ¥ íˆíŠ¸ë§µ ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        
        # í”¼ë²— í…Œì´ë¸” ìƒì„± (ì‹œë‚˜ë¦¬ì˜¤ x í´ë¼ì´ì–¸íŠ¸ ìˆ˜)
        pivot_data = df.pivot_table(
            values=['packets_per_sec', 'p95_latency_ms', 'cpu_usage_percent', 'overall_score'],
            index='scenario',
            columns='client_count',
            aggfunc='mean'
        )
        
        fig, axes = plt.subplots(2, 2, figsize=(20, 12))
        
        # 1. ì²˜ë¦¬ëŸ‰ íˆíŠ¸ë§µ
        sns.heatmap(pivot_data['packets_per_sec'], annot=True, fmt='.0f', 
                   cmap='Blues', ax=axes[0,0], cbar_kws={'label': 'Packets/sec'})
        axes[0,0].set_title('Throughput Heatmap (Packets per Second)')
        axes[0,0].set_xlabel('Client Count')
        axes[0,0].set_ylabel('Scenario')
        
        # 2. ì§€ì—°ì‹œê°„ íˆíŠ¸ë§µ
        sns.heatmap(pivot_data['p95_latency_ms'], annot=True, fmt='.1f', 
                   cmap='Reds', ax=axes[0,1], cbar_kws={'label': 'ms'})
        axes[0,1].set_title('P95 Latency Heatmap (ms)')
        axes[0,1].set_xlabel('Client Count')
        axes[0,1].set_ylabel('Scenario')
        
        # 3. CPU ì‚¬ìš©ë¥  íˆíŠ¸ë§µ
        sns.heatmap(pivot_data['cpu_usage_percent'], annot=True, fmt='.1f', 
                   cmap='Oranges', ax=axes[1,0], cbar_kws={'label': '%'})
        axes[1,0].set_title('CPU Usage Heatmap (%)')
        axes[1,0].set_xlabel('Client Count')
        axes[1,0].set_ylabel('Scenario')
        
        # 4. ì¢…í•© ì ìˆ˜ íˆíŠ¸ë§µ
        sns.heatmap(pivot_data['overall_score'], annot=True, fmt='.0f', 
                   cmap='Greens', ax=axes[1,1], cbar_kws={'label': 'Score'})
        axes[1,1].set_title('Overall Performance Score Heatmap')
        axes[1,1].set_xlabel('Client Count')
        axes[1,1].set_ylabel('Scenario')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/performance_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ì„±ëŠ¥ íˆíŠ¸ë§µ ì €ì¥: {output_dir}/performance_heatmap.png")
    
    def create_bottleneck_analysis(self, df, output_dir='performance_charts'):
        """ë³‘ëª© ì§€ì  ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        
        plt.figure(figsize=(18, 12))
        
        # 1. ì²˜ë¦¬ëŸ‰ íš¨ìœ¨ì„± ë¶„ì„
        plt.subplot(2, 3, 1)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            # í´ë¼ì´ì–¸íŠ¸ë‹¹ ì²˜ë¦¬ëŸ‰ ê³„ì‚°
            throughput_per_client = scenario_data['packets_per_sec'] / scenario_data['client_count']
            plt.plot(scenario_data['client_count'], throughput_per_client, 
                    marker='o', label=f'{scenario.title()}', linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('Packets per Second per Client')
        plt.title('Throughput Efficiency\n(Lower = Bottleneck)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. ì§€ì—°ì‹œê°„ ì¦ê°€ìœ¨ ë¶„ì„
        plt.subplot(2, 3, 2)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            if len(scenario_data) > 1:
                latency_increase = scenario_data['p95_latency_ms'].pct_change() * 100
                plt.plot(scenario_data['client_count'].iloc[1:], latency_increase.iloc[1:], 
                        marker='s', label=f'{scenario.title()}', linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('P95 Latency Increase (%)')
        plt.title('Latency Degradation Rate\n(Higher = Bottleneck)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  vs ì„±ëŠ¥
        plt.subplot(2, 3, 3)
        colors = plt.cm.viridis(np.linspace(0, 1, len(df)))
        scatter = plt.scatter(df['cpu_usage_percent'], df['packets_per_sec'], 
                            c=df['client_count'], cmap='viridis', alpha=0.7, s=100)
        plt.xlabel('CPU Usage (%)')
        plt.ylabel('Packets per Second')
        plt.title('Resource Usage vs Performance')
        plt.colorbar(scatter, label='Client Count')
        plt.grid(True, alpha=0.3)
        
        # 4. ì˜¤ë¥˜ìœ¨ vs ë¶€í•˜
        plt.subplot(2, 3, 4)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            plt.plot(scenario_data['client_count'], scenario_data['error_rate_percent'], 
                    marker='^', label=f'{scenario.title()}', linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('Error Rate (%)')
        plt.title('Error Rate vs Load\n(Higher = System Stress)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        
        # 5. ë©”ëª¨ë¦¬ vs ì§€ì—°ì‹œê°„ ìƒê´€ê´€ê³„
        plt.subplot(2, 3, 5)
        scatter = plt.scatter(df['memory_usage_percent'], df['p95_latency_ms'], 
                            c=df['client_count'], cmap='plasma', alpha=0.7, s=100)
        plt.xlabel('Memory Usage (%)')
        plt.ylabel('P95 Latency (ms)')
        plt.title('Memory vs Latency Correlation')
        plt.colorbar(scatter, label='Client Count')
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        
        # 6. ì„±ëŠ¥ ì €í•˜ ì„ê³„ì  ë¶„ì„
        plt.subplot(2, 3, 6)
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario].sort_values('client_count')
            # ì„±ëŠ¥ ì ìˆ˜ì˜ ê°ì†Œìœ¨
            if len(scenario_data) > 1:
                score_change = scenario_data['overall_score'].diff()
                plt.plot(scenario_data['client_count'].iloc[1:], score_change.iloc[1:], 
                        marker='*', markersize=8, label=f'{scenario.title()}', linewidth=2)
        
        plt.xlabel('Client Count')
        plt.ylabel('Performance Score Change')
        plt.title('Performance Degradation Points\n(Negative = Degradation)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/bottleneck_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ë³‘ëª© ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {output_dir}/bottleneck_analysis.png")
    
    def generate_performance_report(self, df, output_dir='performance_charts'):
        """ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        
        report_file = f"{output_dir}/performance_analysis_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# CppMMO ì„œë²„ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ì „ì²´ ìš”ì•½
            f.write("## ì „ì²´ ìš”ì•½\n\n")
            f.write(f"- ë¶„ì„ëœ í…ŒìŠ¤íŠ¸ ìˆ˜: {len(df)}\n")
            f.write(f"- í…ŒìŠ¤íŠ¸ëœ ì‹œë‚˜ë¦¬ì˜¤: {', '.join(df['scenario'].unique())}\n")
            f.write(f"- í´ë¼ì´ì–¸íŠ¸ ìˆ˜ ë²”ìœ„: {df['client_count'].min()} ~ {df['client_count'].max()}\n\n")
            
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„
            for scenario in df['scenario'].unique():
                scenario_data = df[df['scenario'] == scenario]
                f.write(f"## {scenario.title()} ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„\n\n")
                
                # ìµœê³  ì„±ëŠ¥ ì§€ì 
                best_performance = scenario_data.loc[scenario_data['overall_score'].idxmax()]
                f.write(f"### ìµœê³  ì„±ëŠ¥ ì§€ì \n")
                f.write(f"- í´ë¼ì´ì–¸íŠ¸ ìˆ˜: {best_performance['client_count']}\n")
                f.write(f"- ì¢…í•© ì ìˆ˜: {best_performance['overall_score']:.1f}/100\n")
                f.write(f"- ì²˜ë¦¬ëŸ‰: {best_performance['packets_per_sec']:.1f} packets/sec\n")
                f.write(f"- P95 ì§€ì—°ì‹œê°„: {best_performance['p95_latency_ms']:.2f} ms\n\n")
                
                # ì„±ëŠ¥ ì €í•˜ ì‹œì‘ì  ì°¾ê¸°
                sorted_data = scenario_data.sort_values('client_count')
                if len(sorted_data) > 1:
                    score_diff = sorted_data['overall_score'].diff()
                    degradation_start = sorted_data[score_diff < -5]  # 5ì  ì´ìƒ ê°ì†Œ
                    
                    if not degradation_start.empty:
                        first_degradation = degradation_start.iloc[0]
                        f.write(f"### ì„±ëŠ¥ ì €í•˜ ì‹œì‘ì \n")
                        f.write(f"- í´ë¼ì´ì–¸íŠ¸ ìˆ˜: {first_degradation['client_count']}\n")
                        f.write(f"- CPU ì‚¬ìš©ë¥ : {first_degradation['cpu_usage_percent']:.1f}%\n")
                        f.write(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {first_degradation['memory_usage_percent']:.1f}%\n\n")
                
                # ê¶Œì¥ ìµœëŒ€ í´ë¼ì´ì–¸íŠ¸ ìˆ˜
                stable_performance = scenario_data[
                    (scenario_data['error_rate_percent'] < 1.0) & 
                    (scenario_data['p95_latency_ms'] < 100.0) &
                    (scenario_data['connection_success_rate'] > 95.0)
                ]
                
                if not stable_performance.empty:
                    max_recommended = stable_performance['client_count'].max()
                    f.write(f"### ê¶Œì¥ ìµœëŒ€ í´ë¼ì´ì–¸íŠ¸ ìˆ˜\n")
                    f.write(f"- ì•ˆì •ì  ìš´ì˜ ê°€ëŠ¥: {max_recommended}ëª…\n")
                    f.write(f"- ì¡°ê±´: ì˜¤ë¥˜ìœ¨ < 1%, P95 ì§€ì—°ì‹œê°„ < 100ms, ì—°ê²° ì„±ê³µë¥  > 95%\n\n")
                
                f.write("---\n\n")
            
            # ê°œì„  ê¶Œì¥ì‚¬í•­
            f.write("## ê°œì„  ê¶Œì¥ì‚¬í•­\n\n")
            
            # CPU ë³‘ëª© ì²´í¬
            high_cpu = df[df['cpu_usage_percent'] > 80]
            if not high_cpu.empty:
                f.write("### CPU ìµœì í™” í•„ìš”\n")
                f.write(f"- {len(high_cpu)}ê°œ í…ŒìŠ¤íŠ¸ì—ì„œ CPU ì‚¬ìš©ë¥  > 80%\n")
                f.write("- ê¶Œì¥ì‚¬í•­: í”„ë¡œíŒŒì¼ë§ì„ í†µí•œ í•«ìŠ¤íŒŸ ìµœì í™”, ë©€í‹°ìŠ¤ë ˆë”© ê°œì„ \n\n")
            
            # ë©”ëª¨ë¦¬ ë³‘ëª© ì²´í¬
            high_memory = df[df['memory_usage_percent'] > 80]
            if not high_memory.empty:
                f.write("### ë©”ëª¨ë¦¬ ìµœì í™” í•„ìš”\n")
                f.write(f"- {len(high_memory)}ê°œ í…ŒìŠ¤íŠ¸ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  > 80%\n")
                f.write("- ê¶Œì¥ì‚¬í•­: ë©”ëª¨ë¦¬ í’€ ë„ì…, ë¶ˆí•„ìš”í•œ í• ë‹¹ ìµœì†Œí™”\n\n")
            
            # ì§€ì—°ì‹œê°„ ê°œì„ 
            high_latency = df[df['p95_latency_ms'] > 100]
            if not high_latency.empty:
                f.write("### ì§€ì—°ì‹œê°„ ê°œì„  í•„ìš”\n")
                f.write(f"- {len(high_latency)}ê°œ í…ŒìŠ¤íŠ¸ì—ì„œ P95 ì§€ì—°ì‹œê°„ > 100ms\n")
                f.write("- ê¶Œì¥ì‚¬í•­: ë„¤íŠ¸ì›Œí¬ ë²„í¼ íŠœë‹, ì•Œê³ ë¦¬ì¦˜ ìµœì í™”, ìºì‹± ë„ì…\n\n")
            
            # ì•ˆì •ì„± ê°œì„ 
            low_reliability = df[df['connection_success_rate'] < 95]
            if not low_reliability.empty:
                f.write("### ì—°ê²° ì•ˆì •ì„± ê°œì„  í•„ìš”\n")
                f.write(f"- {len(low_reliability)}ê°œ í…ŒìŠ¤íŠ¸ì—ì„œ ì—°ê²° ì„±ê³µë¥  < 95%\n")
                f.write("- ê¶Œì¥ì‚¬í•­: íƒ€ì„ì•„ì›ƒ ì¡°ì •, ì¬ì—°ê²° ë¡œì§ ê°•í™”, ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ \n\n")
        
        print(f"ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    
    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("CppMMO ì„œë²„ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘...")
        
        # ë°ì´í„° ë¡œë“œ
        self.load_performance_data()
        self.load_csv_data()
        
        if not self.performance_data:
            print("ë¶„ì„í•  ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”:")
            print("python comprehensive_performance_test.py --scenario basic")
            return
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        df = self.extract_performance_metrics()
        
        if df.empty:
            print("ë©”íŠ¸ë¦­ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        print(f"ì¶”ì¶œëœ ë°ì´í„°: {len(df)}ê°œ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("ì‹œë‚˜ë¦¬ì˜¤ë³„ í´ë¼ì´ì–¸íŠ¸ ìˆ˜:")
        for scenario in df['scenario'].unique():
            scenario_data = df[df['scenario'] == scenario]
            client_counts = sorted(scenario_data['client_count'].unique())
            print(f"  {scenario}: {client_counts}")
        
        # ì°¨íŠ¸ ìƒì„±
        output_dir = f"performance_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"\nì°¨íŠ¸ ìƒì„± ì¤‘... (ì €ì¥ ìœ„ì¹˜: {output_dir})")
        self.create_scalability_charts(df, output_dir)
        self.create_performance_heatmap(df, output_dir)
        self.create_bottleneck_analysis(df, output_dir)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        print("ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        self.generate_performance_report(df, output_dir)
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {output_dir}")
        print(f"ğŸ“Š ìƒì„±ëœ ì°¨íŠ¸:")
        print(f"   - scalability_analysis.png")
        print(f"   - performance_heatmap.png") 
        print(f"   - bottleneck_analysis.png")
        print(f"ğŸ“„ ë¶„ì„ ë¦¬í¬íŠ¸: performance_analysis_report.md")
        
        return df, output_dir

def main():
    parser = argparse.ArgumentParser(description='CppMMO ì„œë²„ ì„±ëŠ¥ ë¶„ì„')
    parser.add_argument('--data-dir', '-d', 
                        default='.',
                        help='ì„±ëŠ¥ ë°ì´í„° íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬')
    parser.add_argument('--generate-test-data', 
                        action='store_true',
                        help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    if args.generate_test_data:
        print("ë‹¤ì–‘í•œ í´ë¼ì´ì–¸íŠ¸ ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:\n")
        
        client_counts = [10, 25, 50, 100, 200, 300, 500]
        scenarios = ['basic', 'stress']
        
        for scenario in scenarios:
            print(f"# {scenario.title()} ì‹œë‚˜ë¦¬ì˜¤")
            for count in client_counts:
                # ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ì„¤ì • ì¡°ì •
                duration = 120 if scenario == 'basic' else 180
                print(f"# {count}ëª… í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸")
                print(f"python comprehensive_performance_test.py --scenario {scenario}")
                print("sleep 30  # ì„œë²„ ì•ˆì •í™” ëŒ€ê¸°")
                print()
        
        print("ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œëœ í›„:")
        print("python performance_analysis.py")
        return
    
    analyzer = PerformanceAnalyzer(args.data_dir)
    analyzer.run_analysis()

if __name__ == "__main__":
    main()